"use strict";(globalThis.webpackChunkhumanoid_robotics_training=globalThis.webpackChunkhumanoid_robotics_training||[]).push([[165],{4431(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-3-ai-brain/perception","title":"Perception and VSLAM Concepts: Giving Robots Sight and Spatial Awareness","description":"Understanding Robotic Perception","source":"@site/docs/module-3-ai-brain/perception.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/perception","permalink":"/docs/module-3-ai-brain/perception","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-ai-brain/perception.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim and Synthetic Data Generation","permalink":"/docs/module-3-ai-brain/isaac-sim"},"next":{"title":"Nav2 Navigation Stack: Intelligent Robot Mobility","permalink":"/docs/module-3-ai-brain/navigation"}}');var t=i(4848),r=i(8453);const o={sidebar_position:3},a="Perception and VSLAM Concepts: Giving Robots Sight and Spatial Awareness",l={},c=[{value:"Understanding Robotic Perception",id:"understanding-robotic-perception",level:2},{value:"The Perception Pipeline",id:"the-perception-pipeline",level:3},{value:"Visual Simultaneous Localization and Mapping (VSLAM)",id:"visual-simultaneous-localization-and-mapping-vslam",level:2},{value:"What is VSLAM?",id:"what-is-vslam",level:3},{value:"Biological Analogy",id:"biological-analogy",level:3},{value:"How VSLAM Works",id:"how-vslam-works",level:2},{value:"Key Components",id:"key-components",level:3},{value:"Feature Detection",id:"feature-detection",level:4},{value:"Tracking",id:"tracking",level:4},{value:"Mapping",id:"mapping",level:4},{value:"Localization",id:"localization",level:4},{value:"Mathematical Foundation",id:"mathematical-foundation",level:3},{value:"VSLAM Techniques",id:"vslam-techniques",level:2},{value:"Direct Methods",id:"direct-methods",level:3},{value:"Feature-Based Methods",id:"feature-based-methods",level:3},{value:"Semi-Direct Methods",id:"semi-direct-methods",level:3},{value:"Practical VSLAM Challenges",id:"practical-vslam-challenges",level:2},{value:"Degenerate Conditions",id:"degenerate-conditions",level:3},{value:"Motion Challenges",id:"motion-challenges",level:3},{value:"Computational Constraints",id:"computational-constraints",level:3},{value:"Perception Beyond VSLAM",id:"perception-beyond-vslam",level:2},{value:"Object Recognition",id:"object-recognition",level:3},{value:"Semantic Understanding",id:"semantic-understanding",level:3},{value:"Multi-Modal Perception",id:"multi-modal-perception",level:3},{value:"Applications in Humanoid Robotics",id:"applications-in-humanoid-robotics",level:2},{value:"Navigation",id:"navigation",level:3},{value:"Manipulation",id:"manipulation",level:3},{value:"Human-Robot Interaction",id:"human-robot-interaction",level:3},{value:"Perception in the NVIDIA Isaac Ecosystem",id:"perception-in-the-nvidia-isaac-ecosystem",level:2},{value:"Isaac ROS Packages",id:"isaac-ros-packages",level:3},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Quality Metrics for Perception Systems",id:"quality-metrics-for-perception-systems",level:2},{value:"Accuracy Measures",id:"accuracy-measures",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"perception-and-vslam-concepts-giving-robots-sight-and-spatial-awareness",children:"Perception and VSLAM Concepts: Giving Robots Sight and Spatial Awareness"})}),"\n",(0,t.jsx)(e.h2,{id:"understanding-robotic-perception",children:"Understanding Robotic Perception"}),"\n",(0,t.jsx)(e.p,{children:"Robotic perception is the process by which robots interpret sensory information from their environment to understand the world around them. For humanoid robots navigating complex environments, perception systems act as the eyes and spatial reasoning center, enabling intelligent decision-making and safe navigation."}),"\n",(0,t.jsx)(e.h3,{id:"the-perception-pipeline",children:"The Perception Pipeline"}),"\n",(0,t.jsx)(e.p,{children:"Robotic perception typically follows this pipeline:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Raw Sensors \u2192 Feature Extraction \u2192 Object Recognition \u2192 Scene Understanding \u2192 Action Decision\n"})}),"\n",(0,t.jsx)(e.p,{children:"Each stage builds upon the previous one, transforming raw sensor data into meaningful understanding."}),"\n",(0,t.jsx)(e.h2,{id:"visual-simultaneous-localization-and-mapping-vslam",children:"Visual Simultaneous Localization and Mapping (VSLAM)"}),"\n",(0,t.jsx)(e.h3,{id:"what-is-vslam",children:"What is VSLAM?"}),"\n",(0,t.jsx)(e.p,{children:"VSLAM stands for Visual Simultaneous Localization and Mapping. It's a technique that allows robots to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Map"}),": Create a representation of their environment"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Localize"}),": Determine their position within that environment"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Visual"}),": Using camera imagery as the primary sensor input"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Simultaneously"}),": Performing both tasks at the same time"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"biological-analogy",children:"Biological Analogy"}),"\n",(0,t.jsx)(e.p,{children:"VSLAM mirrors how humans navigate unfamiliar environments:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Visual landmarks"}),": We remember distinctive visual features"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Spatial relationships"}),": We build mental maps of how places relate"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Self-localization"}),": We constantly update our position in the mental map"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Path memory"}),": We remember routes and can return to locations"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"how-vslam-works",children:"How VSLAM Works"}),"\n",(0,t.jsx)(e.h3,{id:"key-components",children:"Key Components"}),"\n",(0,t.jsx)(e.h4,{id:"feature-detection",children:"Feature Detection"}),"\n",(0,t.jsx)(e.p,{children:"VSLAM begins by identifying distinctive visual features in camera images:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Corners"}),": Points where edges intersect"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Edges"}),": Boundaries between different regions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Keypoints"}),": Unique visual patterns"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Descriptors"}),": Mathematical representations of visual features"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"tracking",children:"Tracking"}),"\n",(0,t.jsx)(e.p,{children:"Features are tracked across consecutive frames:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Feature correspondence"}),": Matching features between frames"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Motion estimation"}),": Calculating camera movement from feature changes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal consistency"}),": Maintaining stable feature tracks"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"mapping",children:"Mapping"}),"\n",(0,t.jsx)(e.p,{children:"3D structure is reconstructed from 2D image observations:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Triangulation"}),": Using multiple views to calculate depth"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Bundle adjustment"}),": Optimizing 3D points and camera poses"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Map representation"}),": Storing the reconstructed environment"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"localization",children:"Localization"}),"\n",(0,t.jsx)(e.p,{children:"The robot's position is continuously estimated:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Pose estimation"}),": Calculating 6-DOF position and orientation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Loop closure"}),": Recognizing previously visited locations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Graph optimization"}),": Maintaining globally consistent map"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"mathematical-foundation",children:"Mathematical Foundation"}),"\n",(0,t.jsx)(e.p,{children:"VSLAM solves an optimization problem:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"min \u2211 ||h(p_i, x_j) - z_ij||\n      p,x\n\nwhere:\n- p_i = 3D landmark positions\n- x_j = camera poses\n- z_ij = observed feature measurements\n- h() = projection function mapping 3D to 2D\n"})}),"\n",(0,t.jsx)(e.h2,{id:"vslam-techniques",children:"VSLAM Techniques"}),"\n",(0,t.jsx)(e.h3,{id:"direct-methods",children:"Direct Methods"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Direct alignment"}),": Minimize photometric error between images"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dense reconstruction"}),": Create dense point clouds and depth maps"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Intensity-based"}),": Compare pixel intensities rather than features"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Advantages"}),": Use all image information, work with textureless surfaces"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"feature-based-methods",children:"Feature-Based Methods"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sparse features"}),": Track distinct keypoints across images"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Descriptor matching"}),": Use feature descriptors for robust matching"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Epipolar geometry"}),": Leverage geometric constraints between views"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Advantages"}),": More robust to illumination changes, lower computational cost"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"semi-direct-methods",children:"Semi-Direct Methods"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Tracklets"}),": Short sequences of feature tracks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Optical flow"}),": Dense tracking of sparse features"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hybrid approach"}),": Combine advantages of direct and feature-based methods"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"practical-vslam-challenges",children:"Practical VSLAM Challenges"}),"\n",(0,t.jsx)(e.h3,{id:"degenerate-conditions",children:"Degenerate Conditions"}),"\n",(0,t.jsx)(e.p,{children:"VSLAM faces difficulties in certain environments:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Textureless surfaces"}),": Walls, floors, or objects with insufficient features"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Repetitive patterns"}),": Corridors or structures with similar-looking parts"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic objects"}),": Moving people or vehicles that disrupt tracking"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Low-light conditions"}),": Insufficient illumination for reliable features"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"motion-challenges",children:"Motion Challenges"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fast motion"}),": Blurred images and missed features"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Rotation-only"}),": Pure rotation without translation (no parallax)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Planar motion"}),": Movement in a plane that doesn't reveal depth"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"computational-constraints",children:"Computational Constraints"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Real-time requirements"}),": Processing video streams at high frame rates"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Memory limitations"}),": Storing and optimizing large maps"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Power consumption"}),": Especially important for mobile robots"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"perception-beyond-vslam",children:"Perception Beyond VSLAM"}),"\n",(0,t.jsx)(e.h3,{id:"object-recognition",children:"Object Recognition"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Classification"}),": Identifying object categories (person, chair, door)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Detection"}),": Locating objects within the scene"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Segmentation"}),": Distinguishing object pixels from background"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Tracking"}),": Following objects over time"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"semantic-understanding",children:"Semantic Understanding"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scene classification"}),": Understanding room types or environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Activity recognition"}),": Detecting actions and behaviors"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Relationship inference"}),": Understanding spatial and functional relationships"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Intent prediction"}),": Anticipating future actions of agents"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"multi-modal-perception",children:"Multi-Modal Perception"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"RGB-D integration"}),": Combining color and depth information"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Lidar-camera fusion"}),": Merging geometric and visual data"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Audio-visual"}),": Incorporating sound with visual information"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Haptic integration"}),": Including touch and force feedback"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"applications-in-humanoid-robotics",children:"Applications in Humanoid Robotics"}),"\n",(0,t.jsx)(e.h3,{id:"navigation",children:"Navigation"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safe path planning"}),": Understanding obstacles and free space"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Goal-directed movement"}),": Navigating to specific locations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic avoidance"}),": Responding to moving obstacles"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Human-aware navigation"}),": Safely navigating around people"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"manipulation",children:"Manipulation"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object localization"}),": Precise positioning for grasping"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Grasp planning"}),": Determining how to securely grasp objects"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Workspace understanding"}),": Recognizing surfaces and containers"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task execution"}),": Performing complex manipulation sequences"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"human-robot-interaction",children:"Human-Robot Interaction"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Face detection"}),": Recognizing and tracking human faces"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Gesture recognition"}),": Understanding human commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Emotion detection"}),": Interpreting human expressions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Social navigation"}),": Respecting personal space and social norms"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"perception-in-the-nvidia-isaac-ecosystem",children:"Perception in the NVIDIA Isaac Ecosystem"}),"\n",(0,t.jsx)(e.h3,{id:"isaac-ros-packages",children:"Isaac ROS Packages"}),"\n",(0,t.jsx)(e.p,{children:"NVIDIA Isaac provides perception-focused ROS packages:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac ROS AprilTag"}),": Marker detection and pose estimation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac ROS Stereo DNN"}),": Stereo vision with deep learning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac ROS VSLAM"}),": Visual SLAM implementation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Isaac ROS Manipulator"}),": Perception for manipulation tasks"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"CUDA optimization"}),": Leveraging GPU parallelism"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"TensorRT integration"}),": Optimized neural network inference"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Real-time performance"}),": High-throughput processing"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Power efficiency"}),": Optimized for mobile platforms"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"quality-metrics-for-perception-systems",children:"Quality Metrics for Perception Systems"}),"\n",(0,t.jsx)(e.h3,{id:"accuracy-measures",children:"Accuracy Measures"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Localization accuracy"}),": Position and orientation precision"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Map quality"}),": Completeness and consistency of environment representation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object detection"}),": Precision and recall for identified objects"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robustness"}),": Performance across varied conditions"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Computational efficiency"}),": Processing time and resource usage"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Real-time capability"}),": Meeting timing constraints consistently"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scalability"}),": Performance with increasing environment complexity"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Stability"}),": Consistent operation over extended periods"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Understanding perception and VSLAM concepts is fundamental to creating humanoid robots that can intelligently navigate and interact with their environments using visual information."})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>o,x:()=>a});var s=i(6540);const t={},r=s.createContext(t);function o(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);